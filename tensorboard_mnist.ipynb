{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorboard_mnist.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Lyda8gGqDW3m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import libs \n",
        "\n",
        "import tensorflow as tf # for deep learning \n",
        "\n",
        "# helper library \n",
        "import numpy as np # for matrix maths "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ObQM9GtDW3s",
        "colab_type": "code",
        "outputId": "dfcd63de-3c8f-43fc-93f3-8e3be5b7a97c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "cell_type": "code",
      "source": [
        "# importing dataset \n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets('./data/MNIST/', one_hot=True)\n",
        "\n",
        "# train_df = pd.read_csv('./fashion-mnist_train.csv')\n",
        "\n",
        "# train_df.describe()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-479df6c0512b>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./data/MNIST/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./data/MNIST/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting ./data/MNIST/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./data/MNIST/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MWqvUPOhDW31",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# helper function for conv_layer \n",
        "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
        "    with tf.name_scope(name):\n",
        "        w = tf.Variable(tf.truncated_normal([5,5, size_in, size_out], stddev=0.1), name=\"W\")\n",
        "        b = tf.Variable(tf.truncated_normal([size_out], stddev=0.1), name=\"B\")\n",
        "        \n",
        "        conv = tf.nn.conv2d(input, w, strides=[1,1,1,1], padding=\"SAME\")\n",
        "        act = tf.nn.relu(conv + b)\n",
        "        \n",
        "        # writing summary \n",
        "        tf.summary.histogram(\"weights\", w)\n",
        "        tf.summary.histogram(\"biases\", b)\n",
        "        tf.summary.histogram(\"activation\", act)\n",
        "        \n",
        "        # return a pooled version \n",
        "        return tf.nn.max_pool(act, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oSwv5zIJDW35",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# helper function for fully connected layer \n",
        "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
        "    with tf.name_scope(name):\n",
        "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
        "        b = tf.Variable(tf.truncated_normal([size_out], stddev=0.1), name=\"B\")\n",
        "        \n",
        "        # activation \n",
        "        act = tf.matmul(input, w) + b\n",
        "        \n",
        "        # writing summaries \n",
        "        tf.summary.histogram(\"weights\", w)\n",
        "        tf.summary.histogram(\"biases\", b)\n",
        "        tf.summary.histogram(\"activation\", act)\n",
        "        \n",
        "        return act"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ykEZOxFDW39",
        "colab_type": "code",
        "outputId": "4831772a-5e53-478a-b012-1315fb167940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "# model arctitecture \n",
        "\n",
        "tf.reset_default_graph() # reset all previous graphs\n",
        "sess = tf.Session() # start a session\n",
        "\n",
        "# a placeholder for the images and labels \n",
        "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "x_image = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "# summary for the x_image\n",
        "tf.summary.image(\"input\", x_image, 3)\n",
        "\n",
        "y = tf.placeholder(tf.int32, shape=[None,10])\n",
        "\n",
        "# wiring model\n",
        "# conv layers \n",
        "conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
        "conv2 = conv_layer(conv1, 32, 64, \"conv2\")\n",
        "\n",
        "# flattened layer \n",
        "flattened = tf.reshape(conv2, [-1, 7 * 7 * 64])\n",
        "\n",
        "# making a fully connected layer \n",
        "fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, name=\"fc1\")\n",
        "relu = tf.nn.relu(fc1)\n",
        "\n",
        "# summary for fc layer 1 \n",
        "tf.summary.histogram(\"fc1/relu\", relu)\n",
        "logits = fc_layer(relu,1024, 10, name=\"fc2\")\n",
        "\n",
        "# some important params for the model eval and train \n",
        "\n",
        "with tf.name_scope(\"xent\"):\n",
        "    xent = tf.reduce_mean(\n",
        "        tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=logits, labels=y), name=\"xent\")\n",
        "    tf.summary.scalar(\"xent\", xent)\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    train_step = tf.train.AdamOptimizer(1e-4).minimize(xent)\n",
        "\n",
        "with tf.name_scope(\"accuracy\"):\n",
        "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    tf.summary.scalar(\"accuracy\", accuracy)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-46c8edf5c55d>:35: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_ZOpoq4TDW4D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# merging all the summaries and saving a session graph \n",
        "\n",
        "summ = tf.summary.merge_all()\n",
        "\n",
        "# variables initializer \n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# a file writer for saving summaries \n",
        "writer = tf.summary.FileWriter('./logs')\n",
        "\n",
        "# write session graph\n",
        "writer.add_graph(sess.graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "27KL8qCMDW4H",
        "colab_type": "code",
        "outputId": "092a02af-b5aa-4fe1-b240-414c3310dbc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3434
        }
      },
      "cell_type": "code",
      "source": [
        "# training the model \n",
        "for i in range(2001):\n",
        "    batch = mnist.train.next_batch(100)\n",
        "    if i % 5 == 0:\n",
        "        [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x:batch[0], y:batch[1]})\n",
        "    writer.add_summary(s, i)\n",
        "    if i % 10 == 0:\n",
        "      print(\"accuracy {} at step {}\".format(train_accuracy, i))\n",
        "    \n",
        "    sess.run(train_step, feed_dict={x:batch[0], y:batch[1]})"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.8999999761581421 at step 0\n",
            "accuracy 0.949999988079071 at step 10\n",
            "accuracy 0.9200000166893005 at step 20\n",
            "accuracy 0.9100000262260437 at step 30\n",
            "accuracy 0.9599999785423279 at step 40\n",
            "accuracy 0.949999988079071 at step 50\n",
            "accuracy 0.9200000166893005 at step 60\n",
            "accuracy 0.8999999761581421 at step 70\n",
            "accuracy 0.949999988079071 at step 80\n",
            "accuracy 0.8999999761581421 at step 90\n",
            "accuracy 0.9599999785423279 at step 100\n",
            "accuracy 0.9300000071525574 at step 110\n",
            "accuracy 0.9399999976158142 at step 120\n",
            "accuracy 0.9800000190734863 at step 130\n",
            "accuracy 0.9599999785423279 at step 140\n",
            "accuracy 0.9700000286102295 at step 150\n",
            "accuracy 0.9700000286102295 at step 160\n",
            "accuracy 0.9599999785423279 at step 170\n",
            "accuracy 0.9800000190734863 at step 180\n",
            "accuracy 0.9599999785423279 at step 190\n",
            "accuracy 0.949999988079071 at step 200\n",
            "accuracy 0.9900000095367432 at step 210\n",
            "accuracy 0.9599999785423279 at step 220\n",
            "accuracy 0.9399999976158142 at step 230\n",
            "accuracy 0.9599999785423279 at step 240\n",
            "accuracy 0.9800000190734863 at step 250\n",
            "accuracy 0.9200000166893005 at step 260\n",
            "accuracy 0.9599999785423279 at step 270\n",
            "accuracy 0.9300000071525574 at step 280\n",
            "accuracy 0.9800000190734863 at step 290\n",
            "accuracy 0.9599999785423279 at step 300\n",
            "accuracy 0.9399999976158142 at step 310\n",
            "accuracy 0.9900000095367432 at step 320\n",
            "accuracy 0.9599999785423279 at step 330\n",
            "accuracy 1.0 at step 340\n",
            "accuracy 0.949999988079071 at step 350\n",
            "accuracy 0.9900000095367432 at step 360\n",
            "accuracy 0.9700000286102295 at step 370\n",
            "accuracy 0.9599999785423279 at step 380\n",
            "accuracy 0.9900000095367432 at step 390\n",
            "accuracy 0.9700000286102295 at step 400\n",
            "accuracy 0.9800000190734863 at step 410\n",
            "accuracy 0.949999988079071 at step 420\n",
            "accuracy 0.9900000095367432 at step 430\n",
            "accuracy 0.9700000286102295 at step 440\n",
            "accuracy 0.9800000190734863 at step 450\n",
            "accuracy 0.9900000095367432 at step 460\n",
            "accuracy 0.9800000190734863 at step 470\n",
            "accuracy 0.9900000095367432 at step 480\n",
            "accuracy 0.9900000095367432 at step 490\n",
            "accuracy 0.9900000095367432 at step 500\n",
            "accuracy 0.9599999785423279 at step 510\n",
            "accuracy 0.9900000095367432 at step 520\n",
            "accuracy 0.949999988079071 at step 530\n",
            "accuracy 0.9900000095367432 at step 540\n",
            "accuracy 0.9900000095367432 at step 550\n",
            "accuracy 0.949999988079071 at step 560\n",
            "accuracy 0.9700000286102295 at step 570\n",
            "accuracy 0.9800000190734863 at step 580\n",
            "accuracy 0.9700000286102295 at step 590\n",
            "accuracy 0.9800000190734863 at step 600\n",
            "accuracy 0.9700000286102295 at step 610\n",
            "accuracy 0.9800000190734863 at step 620\n",
            "accuracy 0.9700000286102295 at step 630\n",
            "accuracy 0.9800000190734863 at step 640\n",
            "accuracy 0.9700000286102295 at step 650\n",
            "accuracy 0.9900000095367432 at step 660\n",
            "accuracy 0.9599999785423279 at step 670\n",
            "accuracy 0.949999988079071 at step 680\n",
            "accuracy 0.9800000190734863 at step 690\n",
            "accuracy 0.949999988079071 at step 700\n",
            "accuracy 0.9800000190734863 at step 710\n",
            "accuracy 0.9900000095367432 at step 720\n",
            "accuracy 0.9700000286102295 at step 730\n",
            "accuracy 0.9700000286102295 at step 740\n",
            "accuracy 0.9599999785423279 at step 750\n",
            "accuracy 0.9599999785423279 at step 760\n",
            "accuracy 0.9800000190734863 at step 770\n",
            "accuracy 0.9700000286102295 at step 780\n",
            "accuracy 0.9900000095367432 at step 790\n",
            "accuracy 0.9900000095367432 at step 800\n",
            "accuracy 1.0 at step 810\n",
            "accuracy 0.9800000190734863 at step 820\n",
            "accuracy 0.9399999976158142 at step 830\n",
            "accuracy 0.9900000095367432 at step 840\n",
            "accuracy 0.9900000095367432 at step 850\n",
            "accuracy 0.9599999785423279 at step 860\n",
            "accuracy 0.9700000286102295 at step 870\n",
            "accuracy 1.0 at step 880\n",
            "accuracy 0.9800000190734863 at step 890\n",
            "accuracy 0.9599999785423279 at step 900\n",
            "accuracy 0.9800000190734863 at step 910\n",
            "accuracy 0.9700000286102295 at step 920\n",
            "accuracy 0.9800000190734863 at step 930\n",
            "accuracy 0.949999988079071 at step 940\n",
            "accuracy 0.9700000286102295 at step 950\n",
            "accuracy 1.0 at step 960\n",
            "accuracy 0.9800000190734863 at step 970\n",
            "accuracy 1.0 at step 980\n",
            "accuracy 0.9800000190734863 at step 990\n",
            "accuracy 0.9800000190734863 at step 1000\n",
            "accuracy 1.0 at step 1010\n",
            "accuracy 1.0 at step 1020\n",
            "accuracy 1.0 at step 1030\n",
            "accuracy 0.9800000190734863 at step 1040\n",
            "accuracy 0.9900000095367432 at step 1050\n",
            "accuracy 0.9800000190734863 at step 1060\n",
            "accuracy 0.9900000095367432 at step 1070\n",
            "accuracy 1.0 at step 1080\n",
            "accuracy 0.9599999785423279 at step 1090\n",
            "accuracy 0.9900000095367432 at step 1100\n",
            "accuracy 0.9800000190734863 at step 1110\n",
            "accuracy 0.9800000190734863 at step 1120\n",
            "accuracy 1.0 at step 1130\n",
            "accuracy 1.0 at step 1140\n",
            "accuracy 0.9900000095367432 at step 1150\n",
            "accuracy 0.9900000095367432 at step 1160\n",
            "accuracy 0.9900000095367432 at step 1170\n",
            "accuracy 0.9700000286102295 at step 1180\n",
            "accuracy 0.9599999785423279 at step 1190\n",
            "accuracy 0.9900000095367432 at step 1200\n",
            "accuracy 0.9800000190734863 at step 1210\n",
            "accuracy 0.9900000095367432 at step 1220\n",
            "accuracy 1.0 at step 1230\n",
            "accuracy 0.9700000286102295 at step 1240\n",
            "accuracy 0.9900000095367432 at step 1250\n",
            "accuracy 0.9900000095367432 at step 1260\n",
            "accuracy 1.0 at step 1270\n",
            "accuracy 0.9900000095367432 at step 1280\n",
            "accuracy 1.0 at step 1290\n",
            "accuracy 0.9800000190734863 at step 1300\n",
            "accuracy 0.9900000095367432 at step 1310\n",
            "accuracy 0.9900000095367432 at step 1320\n",
            "accuracy 1.0 at step 1330\n",
            "accuracy 1.0 at step 1340\n",
            "accuracy 0.9700000286102295 at step 1350\n",
            "accuracy 0.9800000190734863 at step 1360\n",
            "accuracy 1.0 at step 1370\n",
            "accuracy 0.9700000286102295 at step 1380\n",
            "accuracy 0.9800000190734863 at step 1390\n",
            "accuracy 0.9900000095367432 at step 1400\n",
            "accuracy 0.9800000190734863 at step 1410\n",
            "accuracy 0.9900000095367432 at step 1420\n",
            "accuracy 0.9599999785423279 at step 1430\n",
            "accuracy 1.0 at step 1440\n",
            "accuracy 1.0 at step 1450\n",
            "accuracy 0.9900000095367432 at step 1460\n",
            "accuracy 0.9800000190734863 at step 1470\n",
            "accuracy 0.9800000190734863 at step 1480\n",
            "accuracy 1.0 at step 1490\n",
            "accuracy 0.9900000095367432 at step 1500\n",
            "accuracy 1.0 at step 1510\n",
            "accuracy 0.9900000095367432 at step 1520\n",
            "accuracy 0.9900000095367432 at step 1530\n",
            "accuracy 0.9800000190734863 at step 1540\n",
            "accuracy 0.9700000286102295 at step 1550\n",
            "accuracy 0.9700000286102295 at step 1560\n",
            "accuracy 0.9700000286102295 at step 1570\n",
            "accuracy 0.9800000190734863 at step 1580\n",
            "accuracy 0.9900000095367432 at step 1590\n",
            "accuracy 0.9599999785423279 at step 1600\n",
            "accuracy 1.0 at step 1610\n",
            "accuracy 0.9800000190734863 at step 1620\n",
            "accuracy 0.9700000286102295 at step 1630\n",
            "accuracy 0.9800000190734863 at step 1640\n",
            "accuracy 1.0 at step 1650\n",
            "accuracy 0.9800000190734863 at step 1660\n",
            "accuracy 1.0 at step 1670\n",
            "accuracy 1.0 at step 1680\n",
            "accuracy 0.9599999785423279 at step 1690\n",
            "accuracy 0.9800000190734863 at step 1700\n",
            "accuracy 0.9900000095367432 at step 1710\n",
            "accuracy 1.0 at step 1720\n",
            "accuracy 0.9800000190734863 at step 1730\n",
            "accuracy 1.0 at step 1740\n",
            "accuracy 0.9900000095367432 at step 1750\n",
            "accuracy 0.9700000286102295 at step 1760\n",
            "accuracy 0.9900000095367432 at step 1770\n",
            "accuracy 0.9900000095367432 at step 1780\n",
            "accuracy 0.9900000095367432 at step 1790\n",
            "accuracy 0.9800000190734863 at step 1800\n",
            "accuracy 0.9900000095367432 at step 1810\n",
            "accuracy 1.0 at step 1820\n",
            "accuracy 0.9800000190734863 at step 1830\n",
            "accuracy 0.9599999785423279 at step 1840\n",
            "accuracy 0.9900000095367432 at step 1850\n",
            "accuracy 1.0 at step 1860\n",
            "accuracy 1.0 at step 1870\n",
            "accuracy 0.9900000095367432 at step 1880\n",
            "accuracy 0.9900000095367432 at step 1890\n",
            "accuracy 0.9800000190734863 at step 1900\n",
            "accuracy 0.9900000095367432 at step 1910\n",
            "accuracy 1.0 at step 1920\n",
            "accuracy 1.0 at step 1930\n",
            "accuracy 1.0 at step 1940\n",
            "accuracy 0.9900000095367432 at step 1950\n",
            "accuracy 1.0 at step 1960\n",
            "accuracy 1.0 at step 1970\n",
            "accuracy 1.0 at step 1980\n",
            "accuracy 0.9900000095367432 at step 1990\n",
            "accuracy 0.9900000095367432 at step 2000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}